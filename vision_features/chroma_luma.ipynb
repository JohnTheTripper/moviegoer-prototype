{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chroma_luma\n",
    "This notebook explores the various chroma (color) and luma (brightness) features that can be extracted from individual frames. We'll be making extensive use of the `OpenCV` computer vision library. Functionality demonstrated here will be replicated in a functions file, and then used to populate the frame-level DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from vision_features_io import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "film = 'parasite'\n",
    "frame = 933\n",
    "frame_folder = os.path.join('../frame_per_second', film)\n",
    "img_path = frame_folder + '/' + film + '_frame' + str(frame) + '.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading an image\n",
    "We can load the image and get its width and height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358, 854, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this frame is 854 (width) x 358 (height) pixels and has a BGR value\n",
    "image = cv2.imread(img_path)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a (width, height) coordinate and get the BGR value, consisting of three color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([53, 74, 71], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image[10][20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blank Frames\n",
    "The easiest frame to spot is the blank frame, with either all black or all white pixels. These could be used to identify scene transitions, such as when a scene ends with a dip-to-black or dip-to-white, transitioning to blank frames before the next scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-black frames\n",
    "We can calculate the mean luminosity of the image by calculating the average of every BGR value for every pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "film = 'parasite'\n",
    "frame = 22\n",
    "frame_folder = os.path.join('../frame_per_second', film)\n",
    "img_path = frame_folder + '/' + film + '_frame' + str(frame) + '.jpg'\n",
    "image = cv2.imread(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# every pixel in this frame has a BGR value of (0, 0, 0)\n",
    "image.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black frame detected\n"
     ]
    }
   ],
   "source": [
    "if image.mean() < 3: # threshold of 3, to be safe\n",
    "    print('black frame detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All-white frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if image.mean() > 252:\n",
    "    print('white frame detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Luma\n",
    "Measurements relating to luma, or single-scale (black-and-white) brightness.\n",
    "### Mean brightness\n",
    "We can take the brightness of the frame by calculating the average brightness of each pixel. But because each pixel is a color BGR element, we must first convert the image to grayscale, because brightness doesn't map linearly to BGR values. After converting each pixels' three BGR values to a single grayscale value, we can take the mean of the pixels' grayscale values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "film = 'booksmart'\n",
    "frame = 4102\n",
    "frame_folder = os.path.join('../frame_per_second', film)\n",
    "img_path = frame_folder + '/' + film + '_frame' + str(frame) + '.jpg'\n",
    "image = cv2.imread(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.32336262926611"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is NOT the proper way to calculate brightness\n",
    "image.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111.82475067757808"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this conversion to grayscale is necessary\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "gray.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum brightness\n",
    "Although black values always sit at the end of the spectrum (0, 0, 0), white values in H.264-compressed videos rarely approach the pure white point of (255, 255, 255) because of gamma correction, a bandwidth-saving technique. On a display, the human eye doesn't need to see pure white to interpret pure white. Brightness is a power-scale, not a linear-scale, and so gamma correction can be used to reduce the amount of data used to convey white.\n",
    "\n",
    "Below, we can search for the frame's brightest pixel. We may use this to scale white values. Remember that when searching for luma intensity, we need to convert to grayscale first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n",
      "(4, 412)\n"
     ]
    }
   ],
   "source": [
    "brightest_pixel = 0\n",
    "brightest_coordinate = 0\n",
    "x = 0\n",
    "\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "for a in gray:\n",
    "    y = 0\n",
    "    for b in a:\n",
    "        if b > brightest_pixel:\n",
    "            brightest_pixel = b\n",
    "            brightest_coordinate = (x, y)\n",
    "        y += 1\n",
    "    x += 1\n",
    "print(brightest_pixel)\n",
    "print(brightest_coordinate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Highlights and Shadows\n",
    "Frames may contain lots of highlights and shadows, pixels that appear all-white or all-black. We may want to disregard these, as they don't contain useful color information.\n",
    "\n",
    "One side effect of discarding these pixels is losing the original `shape` of the frame information. Instead of the (width, height, RGB) shape, we're left with an unrowed (pixel, RGB) array.\n",
    "\n",
    "First, we'll unrow the frame array, and then search for the brightest and darkest pixels. We'll discard pixels that are close in intensity to those brightest and darkest, and we're left with just pixels with good color information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(281230, 3)\n"
     ]
    }
   ],
   "source": [
    "frame = load_frame('booksmart', 3744)\n",
    "brightest = brightest_pixel_intensity(frame)\n",
    "darkest = darkest_pixel_intensity(frame)\n",
    "unrowed = unrow_frame(frame)\n",
    "\n",
    "if brightest >= 200:\n",
    "\n",
    "    highlights_removed_list = []\n",
    "    for pixel in unrowed:\n",
    "        if pixel.mean() < brightest * .90:\n",
    "            highlights_removed_list.append(pixel)\n",
    "\n",
    "    unrowed = np.array(highlights_removed_list)\n",
    "    \n",
    "if darkest <= 15:\n",
    "    shadows_removed_list = []\n",
    "    for pixel in unrowed:\n",
    "        if pixel.mean() > darkest + 10:\n",
    "            shadows_removed_list.append(pixel)\n",
    "\n",
    "    unrowed = np.array(shadows_removed_list)\n",
    "\n",
    "print(unrowed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma\n",
    "Measurements related to chroma, the BGR color information of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting BGR\n",
    "Each pixel has three additive values that make up its intensity and color: one 0-255 value each for Blue, Green, and Red. Although `OpenCV` has functions that can automatically extract things like the frame's mean value for each color channel, this won't work if we've unrowed the image data (such as in the previous code-block)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(281230,)\n",
      "(281230,)\n",
      "(281230,)\n"
     ]
    }
   ],
   "source": [
    "b = []\n",
    "g = []\n",
    "r = []\n",
    "\n",
    "for pixel in unrowed:\n",
    "    b.append(pixel[0])\n",
    "    g.append(pixel[1])\n",
    "    r.append(pixel[2])\n",
    "\n",
    "b = np.array(b)\n",
    "g = np.array(g)\n",
    "r = np.array(r)\n",
    "\n",
    "print(b.shape)\n",
    "print(g.shape)\n",
    "print(r.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dominant Color\n",
    "Generally, most frames have relatively balanced BGR color values. But some frames may have a dominant color, either as an artistic choice (leaning heavily into blue to make a scene feel \"cooler\"), or because of the actual events happening in the scene (an underwater scene).\n",
    "\n",
    "We can determine if frames have predominantly blue, green, or red colors by looking at the intensity of each color as a proportion of the frame's overall intensity. So if any of the three intensities is more than 50% of the total intensity of the image, we declare that the dominant color.\n",
    "\n",
    "BGR (RGB) is an additive color space, and secondary colors are created by mixing two of the primary colors together. So if there was a high intensity of blue and red (which makes magenta), there would be a low intensity of green. If the image has a low proportion of green, less than 10%, we would declare the dominant color to be magenta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = load_frame('booksmart', 4072)\n",
    "mid_pixels = remove_highlights_shadows(frame)\n",
    "b, g, r = bgr(mid_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cyan\n"
     ]
    }
   ],
   "source": [
    "primary_threshold = .5\n",
    "secondary_threshold = .1\n",
    "if frame.mean() < 30:\n",
    "    print('too dark to measure color')\n",
    "elif b.mean() / (mid_pixels.mean() * 3) > primary_threshold:\n",
    "    print('blue')\n",
    "elif g.mean() / (mid_pixels.mean() * 3) > primary_threshold:\n",
    "    print('green')\n",
    "elif r.mean() / (mid_pixels.mean() * 3) > primary_threshold:\n",
    "    print('red')\n",
    "elif b.mean() / (mid_pixels.mean() * 3) < secondary_threshold:\n",
    "    print('yellow')\n",
    "elif g.mean() / (mid_pixels.mean() * 3) < secondary_threshold:\n",
    "    print('magenta')\n",
    "elif r.mean() / (mid_pixels.mean() * 3) < secondary_threshold:\n",
    "    print('cyan')\n",
    "else:\n",
    "    print('no dominant color')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (moviegoer)",
   "language": "python",
   "name": "moviegoer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
